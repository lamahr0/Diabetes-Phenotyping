{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4749a7b5",
   "metadata": {},
   "source": [
    "# Diabetes Phenotyping: A Natural Language Processing-Driven Machine Learning Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffed22",
   "metadata": {},
   "source": [
    "<img src=\"flowchart.png\" alt=\"flowchart\" />\n",
    "Diagnosing diabetes using automated approaches would aid in increasing patient care, disease management and assist clinicians. in this project supervised algorithms were used to develop classifiers developed to detect type 1 diabetes and type 2 diabetes from MIMIC-IV clinical notes using three different approaches by leveraging natural language processing techniques, TD-IDF, word embeddings and the use of pretrained large language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1413eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages imports \n",
    "#preprocessing \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import copy\n",
    "import re\n",
    "import string\n",
    "from flashtext import KeywordProcessor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Evaluation \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "#Approach1&2 models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#Approach 2\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#Approach 3\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "#Visualiziation\n",
    "import matplotlib.pyplot as plt \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc81a6",
   "metadata": {},
   "source": [
    "<h3>Dataset</h3>\n",
    "    \n",
    "the instructions to download the MIMIC-IV database files are on the README file.<br>\n",
    "for additional information about MIMIC Data visit https://physionet.org/content/mimiciv/2.2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8aef4",
   "metadata": {},
   "source": [
    "# Patients Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30a9a7",
   "metadata": {},
   "source": [
    "The Algorithm psudo code and flowchart for this part can be found: <br>\n",
    "Stanford University. Type 1 and type 2 Diabetes Mellitus. PheKB; 2020 Available from: https://phekb.org/phenotype/1506\n",
    "the MIMIC data is not labeled hence the need to do the labeling, the algorithm works as a guide to help identify the phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140b864",
   "metadata": {},
   "source": [
    "<h3>Load Data</h3>\n",
    "<br>\n",
    "-d_items: a table that define the item ids referenced in other files in the ICU folder (similar to a items dictionary or meta data)<br>\n",
    "-charts: a table that contains <br>\n",
    "-prescriptions:a table that contains all patients prescriptions<br>\n",
    "-diagnoses:a table that contains all the patients diagnoses<br>\n",
    "-labs:a table that contain all labs results other than ICU tests<br>\n",
    "-labs_items:a table that define the item ids referenced in the labs table (similar to a items dictionary or meta data) <br>\n",
    "-diagnoses_details: a table that define the item ids referenced in the diagnoses table (similar to a items dictionary or meta data)<br>\n",
    "-notes: a table that contains discharge summary notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3005ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load files from MIMIC folders\n",
    "#ICU folder\n",
    "d_items = pd.read_csv(\"mimic-iv-2.2/icu/d_items.csv\")\n",
    "charts = pd.read_csv(\"mimic-iv-2.2/icu/chartevents.csv\")\n",
    "#HOSP folder\n",
    "prescriptions = pd.read_csv(\"mimic-iv-2.2/hosp/prescriptions.csv\")\n",
    "diagnoses = pd.read_csv(\"mimic-iv-2.2/hosp/diagnoses_icd.csv\")\n",
    "labs = pd.read_csv(\"mimic-iv-2.2/hosp/labevents.csv\")\n",
    "labs_items = pd.read_csv(\"mimic-iv-2.2/hosp/d_labitems.csv\")\n",
    "diagnoses_details = pd.read_csv(\"mimic-iv-2.2/hosp/d_icd_diagnoses.csv\")\n",
    "#MIMIC Note Folder\n",
    "notes = pd.read_csv(\"mimic-iv-note-deidentified-free-text-clinical-notes-2/note/discharge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945916ac",
   "metadata": {},
   "source": [
    "<h3> Diabetes Items IDs Extraction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file that contains the diabetes medictions (generic names and brands).\n",
    "#medictions list was acquired from the Upadhyaya et al paper\n",
    "rx_labels = pd.read_csv(\"Full_Rx_List.csv\").astype(str)\n",
    "rx_labels = rx_labels.stack().tolist()\n",
    "\n",
    "#get the records that have a category of labs and they contain glucose (blood sugar) as labels from the \n",
    "#ICU items dictionary \n",
    "icu_labs_labels = d_items[d_items.label.str.contains('Glucose',case=False) & \n",
    "                          d_items.category.str.contains('labs',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9541359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get glucose labs  \n",
    "glucose_lab_codes = [50809,50931,52569,52027] \n",
    "glucose_labs_labels = labs_items[labs_items.itemid.isin(glucose_lab_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the ICD9 and ICD10 codes for diabetes \n",
    "\n",
    "#ICD9 Diabetes codes start with 250 so extract the codes that match this condition\n",
    "icd9_label = diagnoses_details[diagnoses_details.icd_code.str.contains('^250' , case=False ,regex=True) \n",
    "& diagnoses_details.long_title.str.contains(r'(?i)\\Diabetes\\b',case=False ,regex=True)]\n",
    "\n",
    "#ICD10 Diabetes codes start with E10 or E11 so extract the codes that match this condition\n",
    "icd10_label1 = diagnoses_details[diagnoses_details.icd_code.str.contains('^E11', case=False ,regex=True) \n",
    "& diagnoses_details.long_title.str.contains(r'(?i)\\Diabetes\\b',case=False ,regex=True)]\n",
    "icd10_label2 =diagnoses_details[diagnoses_details.icd_code.str.contains('^E10',  case=False ,regex=True) \n",
    "& diagnoses_details.long_title.str.contains(r'(?i)\\Diabetes\\b',case=False ,regex=True)]\n",
    "\n",
    "#merge all diabetes ICD codes in one dataframe\n",
    "diabetes_codes_labels = pd.concat([icd9_label, icd10_label1 ,icd10_label2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc5bcc",
   "metadata": {},
   "source": [
    "<h3>Get Patients with diabetes diagonsis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28393c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter patients diagnoses only with diabetes diagnosis codes \n",
    "diabetes_diagnoses = diagnoses[diagnoses.icd_code.isin(diabetes_codes_labels.icd_code)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db2c42",
   "metadata": {},
   "source": [
    "<h3> Filter diabetes medictions and prescriptions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter_meds(row ,meds):\n",
    "    '''\n",
    "    check if the row matchs any items from the medictions list \n",
    " \n",
    "    Args:\n",
    "        row (str): dataframe row \n",
    "        meds (str list): A list of medictions \n",
    " \n",
    "    Returns:\n",
    "        1: if the mediction name matches any of the list items\n",
    "        0: if the mediction name doesn't match any of the list items\n",
    "    \n",
    "    '''\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    #add the meds as keywords to be searched for \n",
    "    keyword_processor.add_keywords_from_list(meds)\n",
    "    if(keyword_processor.extract_keywords(row)):\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#fill the null values with unknown and convert the drug column type into string so it can be filtered\n",
    "prescriptions.drug.fillna('unknown' , inplace=True)\n",
    "prescriptions.drug = prescriptions.drug.astype(str)\n",
    "\n",
    "#apply filter medictions function on the prescriptions table which flags the diabetes prescriptions \n",
    "prescriptions['is_diabetes_med'] = prescriptions.drug.apply(Filter_meds , args=(rx_labels,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe that contain only diabetes prescriptions \n",
    "diabetes_prescriptions = prescriptions[prescriptions['is_diabetes_med'] == 1]\n",
    "\n",
    "#remove insulin for Hyperkalemia as it's not for diabetes\n",
    "diabetes_prescriptions= diabetes_prescriptions[~diabetes_prescriptions.drug.str.contains(\"Hyperkalemia\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag the prescriptions that are considered a metformin prescription (brands and generic names)\n",
    "metformin_brands = ['Fortamet','Glucophage','Glumetza','Riomet','Metformin']\n",
    "diabetes_prescriptions['is_metformin'] = diabetes_prescriptions.drug.apply(Filter_meds,args=(metformin_brands,))\n",
    "\n",
    "#create a new dataframe with oral hypoglycemic prescriptions other than metformin\n",
    "non_metformin_diabetes_prescriptions = diabetes_prescriptions[diabetes_prescriptions['is_metformin'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5b7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#view diabetes prescriptions\n",
    "diabetes_prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb47f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#view diabetes prescriptions which are not metformin to be used in later stages of the filtering\n",
    "non_metformin_diabetes_prescriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25b691",
   "metadata": {},
   "source": [
    "<h3>Filter diabetes labs results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get glucose labs results only \n",
    "diabetes_glucose_labs = labs[labs.itemid.isin(glucose_labs_labels.itemid)]\n",
    "\n",
    "#get icu glucose labs results only\n",
    "icu_charts_labs_results = charts[charts.itemid.isin(icu_labs_labels.itemid)]\n",
    "\n",
    "#get A1C labs results only\n",
    "diabetes_A1C_labs = labs[labs.itemid == 50852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d51c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glucose_abnormal_filtering(diagnosed_patient , diabetes_glucose_labs):\n",
    "    '''\n",
    "    checks if the patient glucose labs results are abnormal (abnormal range starts from 125mg/dl)\n",
    " \n",
    "    Args:\n",
    "        diagnosed_patient (df row): the record of a diagnosed patient \n",
    "        diabetes_glucose_labs (df):  a dataframe that contains lab tests details\n",
    " \n",
    "    Returns:\n",
    "        1: if the patient has an abnormal test result \n",
    "        0: if the patient doesnt have an abnormal test result\n",
    "    \n",
    "    '''\n",
    "    patient_id = diagnosed_patient.subject_id\n",
    "    #select the patient lab results\n",
    "    patient_labs_results = diabetes_glucose_labs[diabetes_glucose_labs.subject_id == patient_id]\n",
    "    #check if the results are abnormal\n",
    "    if(patient_labs_results.valuenum >= 125).any():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "def A1C_abnormal_filtering(diagnosed_patient , diabetes_A1C_labs):\n",
    "    '''\n",
    "    checks if the patient A1C Hemoglobin labs results are abnormal (abnormal range starts from 6.5)\n",
    " \n",
    "    Args:\n",
    "        diagnosed_patient (df): the record of a diagnosed patient \n",
    "        diabetes_A1C_labs (df):  a dataframe that contains lab tests details\n",
    " \n",
    "    Returns:\n",
    "        1: if the patient has an abnormal test result \n",
    "        0: if the patient doesnt have an abnormal test result\n",
    "    \n",
    "    '''\n",
    "    patient_id = diagnosed_patient.subject_id\n",
    "    #select the patients lab results\n",
    "    patient_labs_results = diabetes_A1C_labs[diabetes_A1C_labs.subject_id == patient_id]\n",
    "    #check if the results are abnormal\n",
    "    if(patient_labs_results.valuenum >= 6.5).any():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a deepcopy of the diabetes diagnoses dataframe in order to create new columns \n",
    "#a deepcopy ensure that the exception \"SettingWithCopyWarning\" is not raised \n",
    "filtered_diagnosed_patients = copy.deepcopy(diabetes_diagnoses)\n",
    "\n",
    "#create a new column that flags whether a patient have a diabetes prescription or not \n",
    "filtered_diagnosed_patients['take_diabetes_meds'] = diabetes_diagnoses['subject_id'].isin(\n",
    "    diabetes_prescriptions['subject_id']).astype(int)\n",
    "\n",
    "#create a new column that flag whether a patient have a patient have an abnormal glucose lab or not\n",
    "filtered_diagnosed_patients['abnormal_glucose_lab'] = diabetes_diagnoses.apply(Glucose_abnormal_filtering ,\n",
    "    axis=1, args=(diabetes_glucose_labs, ))\n",
    "\n",
    "#create a new column that flag whether a patient have a patient have an abnormal glucose ICU lab or not                                                                              )\n",
    "filtered_diagnosed_patients['abnormal_ICU_lab'] = diabetes_diagnoses.apply(Glucose_abnormal_filtering ,\n",
    "    axis=1, args=(icu_charts_labs_results, ))\n",
    "\n",
    "#create a new column that flag whether a patient have a patient have an abnormal A1C lab or not\n",
    "filtered_diagnosed_patients['abnormal_A1C_lab'] = diabetes_diagnoses.apply(A1C_abnormal_filtering,\n",
    "    axis=1, args=(diabetes_A1C_labs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd428be4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the labs after filtering \n",
    "filtered_diagnosed_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84143788",
   "metadata": {},
   "source": [
    "<h3>Filter diabetes patients</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diabetes_cohort(patient_row):\n",
    "    '''\n",
    "    dtermines if the patient is a diabetes patient or not \n",
    " \n",
    "    Args:\n",
    "        patient_row (df row): the record of a diagnosed patient \n",
    " \n",
    "    Returns:\n",
    "        1: if the patient is diabetes patient \n",
    "        0: if the patient is not consdiered as diabetes patient\n",
    "    \n",
    "    '''\n",
    "    #if the patient is taking a diabetes med return 1\n",
    "    if(patient_row.take_diabetes_meds == 1):\n",
    "        return 1\n",
    "    #if the patient have an abnormal labs results return 1\n",
    "    elif(patient_row.abnormal_glucose_lab == 1 | patient_row.abnormal_A1C_lab == 1 \n",
    "    | patient_row.abnormal_ICU_lab == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a deepcopy of the dataframe to create a new column.\n",
    "diagnosed_patients_cohort = copy.deepcopy(filtered_diagnosed_patients) \n",
    "\n",
    "#create a new column that flags diabetes patients through applying a function\n",
    "diagnosed_patients_cohort['has_diabetes'] = filtered_diagnosed_patients.apply(Diabetes_cohort,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataframe of diabetes patients cohort \n",
    "diagnosed_patients_cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069a16b",
   "metadata": {},
   "source": [
    "<h3> Label Type 1 diabetes or Type 2 diabetes </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc3190",
   "metadata": {},
   "source": [
    "After Identifying the diabetes patients group, the next step is to identify which diabetes phenotype the have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines type 1 diabetes and type 2 diabetes ICD codes.\n",
    "#reference:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10283086/\n",
    "Type1DM_icd_codes =  ['^250[0-9]1$' , '^250[0-9]3$' , '^E10']\n",
    "Type2DM_icd_codes =  ['^250[0-9]0$' , '^250[0-9]2$' , '^E11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the frequency of type 1 diabetes ICD code from each patient \n",
    "diagnosed_patients_cohort['DMT1_Code_Frequency'] = diagnosed_patients_cohort.subject_id.where(\n",
    "    diagnosed_patients_cohort['icd_code'].str.contains('|'.join(Type1DM_icd_codes))).groupby(\n",
    "    diagnosed_patients_cohort['subject_id']).transform('count')\n",
    "\n",
    "#calculate the frequency of type 2 diabetes ICD code from each patient \n",
    "diagnosed_patients_cohort['DMT2_Code_Frequency'] =diagnosed_patients_cohort.subject_id.where(\n",
    "    diagnosed_patients_cohort['icd_code'].str.contains('|'.join(Type2DM_icd_codes))).groupby(\n",
    "    diagnosed_patients_cohort['subject_id']).transform('count')\n",
    "\n",
    "#calculate the ratio of type 1 codes to type 2 codes for each patient \n",
    "diagnosed_patients_cohort['DM1_DM2_Ratio'] = (diagnosed_patients_cohort.DMT1_Code_Frequency \n",
    "                                              / diagnosed_patients_cohort.DMT2_Code_Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates to the unique IDs of diabetes patients\n",
    "diagnosed_patients_cohort.drop_duplicates(subset=['subject_id'] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b4c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get glucagon prescriptions only \n",
    "glucagon_rx = diabetes_prescriptions[diabetes_prescriptions.drug.str.contains\n",
    "                                     (r'(?i)\\bglucagon\\b' ,regex=True, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f04ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag with 1 if the patient have a glucagon prescription\n",
    "diagnosed_patients_cohort['glucagon_rx'] =  np.where(diagnosed_patients_cohort.subject_id.isin(\n",
    "    glucagon_rx.subject_id),1 , 0)\n",
    "\n",
    "#flag with 1 if the patient have any oral hypoglycemic medications other than metformin  \n",
    "diagnosed_patients_cohort['non_metformin_meds'] =  np.where(diagnosed_patients_cohort.subject_id.isin(\n",
    "    non_metformin_diabetes_prescriptions.subject_id),1 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diabetes_Type_labeling(row):\n",
    "    '''\n",
    "    determines the phenotype of the dibaetes patient \n",
    " \n",
    "    Args:\n",
    "        row (df row): the record of a diagnosed patient \n",
    " \n",
    "    Returns:\n",
    "        0: if the patient doesn't meet the requirements for the phenotypes\n",
    "        1: if the patient is type 1 diabetes patient \n",
    "        2: if the patient is type 2 diabetes patient\n",
    "    \n",
    "    '''\n",
    "    if(row.DM1_DM2_Ratio <= 0.5):\n",
    "        return 2 \n",
    "    elif(row.DM1_DM2_Ratio > 0.5 and row.glucagon_rx == 1):\n",
    "        return 1\n",
    "    elif(row.DM1_DM2_Ratio > 0.5 and row.glucagon_rx == 0 & row.non_metformin_meds ==0):\n",
    "        return 1\n",
    "    elif(row.DM1_DM2_Ratio > 0.5 and row.glucagon_rx == 0 & row.non_metformin_meds ==1):\n",
    "        return 2\n",
    "    else: \n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the diabetes phenotype label of each patient \n",
    "diagnosed_patients_cohort['Label'] = diagnosed_patients_cohort.apply(Diabetes_Type_labeling, axis = 1)\n",
    "\n",
    "#get diabetes type 1 patients\n",
    "diabetesT1_patients = diagnosed_patients_cohort[diagnosed_patients_cohort.Label==1]\n",
    "\n",
    "#get diabetes type 2 patients\n",
    "diabetesT2_patients = diagnosed_patients_cohort[diagnosed_patients_cohort.Label==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd68b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the tyep 1 patients , 2344 patients Identified\n",
    "diabetesT1_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the tyep 2 patients , 34060 patients Identified\n",
    "diabetesT2_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393f11c",
   "metadata": {},
   "source": [
    "<h3> Filter, label and clean the discharge notes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter_notes(note):\n",
    "    '''\n",
    "    filter the note based on  common terms for diabetes according to diabetes.org. \n",
    "\n",
    "    Args:\n",
    "        note (str): a patient clinical note  \n",
    " \n",
    "    Returns:\n",
    "        0: if the note doesn't contain any terms \n",
    "        1: if a term was found in the note\n",
    "    \n",
    "    '''\n",
    "    #list of the common words related to diabetes. ref:https://diabetes.org/about-diabetes/common-terms\n",
    "    words = ['diabetes','diabetic''insulin', 'sugar','A1C','glucose','hyperglycemia','hypoglycemia',\n",
    "             'Euglycemia','diabetic', 'diabet','Fasting', 'hypoglycem','hypoglycemic','pancreas'] \n",
    "    keyword_processor = KeywordProcessor()\n",
    "    #add the words to the KeywordProcessor dictionary to be searched\n",
    "    keyword_processor.add_keywords_from_list(words)\n",
    "    #search the note for the diabets common terms\n",
    "    if(keyword_processor.extract_keywords(note)):\n",
    "        #if found return 1\n",
    "        return 1 \n",
    "    else:\n",
    "        #if not found return 0\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the notes to get only type 1 diabetes patients notes\n",
    "notes_T1 = notes[notes.subject_id.isin(diabetesT1_patients.subject_id)]\n",
    "#filter the notes again to get only the relevant patient clinical notes \n",
    "notes_T1['is_diabetes_related'] = notes_T1.text.apply(Filter_notes)\n",
    "notes_T1 = notes_T1[notes_T1.is_diabetes_related == 1]\n",
    "#get a sample of 7000 notes\n",
    "notes_T1 = notes_T1.sample(n=7000, random_state=42)\n",
    "#assign the label 1 for these notes to be used for binary classifcation\n",
    "notes_T1['label'] = 1\n",
    "#filter the notes to get only type 2 diabetes patients notes\n",
    "notes_T2 = notes[notes.subject_id.isin(diabetesT2_patients.subject_id)]\n",
    "#filter the notes again to get only the relevant patient clinical notes \n",
    "notes_T2['is_diabetes_related'] = notes_T2.text.apply(Filter_notes)\n",
    "notes_T2 = notes_T2[notes_T2.is_diabetes_related == 1]\n",
    "#get a sample of 7000 notes\n",
    "notes_T2 = notes_T2.sample(n=7000, random_state=42)\n",
    "#assign the label 1 for these notes to be used for binary classifcation\n",
    "notes_T2['label'] = 0 \n",
    "\n",
    "#merge the notes \n",
    "labeled_notes = pd.concat([notes_T1, notes_T2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index as it was shffuled in the previous steps\n",
    "labeled_notes.reset_index(inplace=True)\n",
    "labeled_notes = labeled_notes[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac28a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the labeled notes dataframe\n",
    "labeled_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning_notes(text):\n",
    "    '''\n",
    "    preprocess and clean text in order to be used in modeling\n",
    "\n",
    "    Args:\n",
    "        text (str): a note to be cleaned\n",
    " \n",
    "    Returns:\n",
    "        cleaned_text: preprocessed and cleaned text\n",
    "    \n",
    "    '''\n",
    "    #remove new lines \n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    #remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    #remove special characters\n",
    "    text = re.sub(r\"[-()\\\"#/&@$;Ã—+:<>{}`'+=~|.!?,]\",'', text) \n",
    "    #remove digits\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'\\d+','', text.lower())\n",
    "    #remove spaces with more than 3 spaces\n",
    "    text = re.sub(r'\\s{3,}', ' ', text)\n",
    "    #remove the header of the notes \n",
    "    header_words = ['name', 'unit', 'admission', 'allergy', 'date', 'discharge' ,'birth', 'sex', 'service']\n",
    "    for word in header_words:\n",
    "        text = re.sub(re.escape(word), '', text)\n",
    "    #remove spaces \n",
    "    text = text.strip()\n",
    "    #tokenize the words\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    #remove stop words\n",
    "    tokenized_words = [w for w in tokenized_words if w not in stopwords.words(\"english\")]\n",
    "    #lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    #keep tokens that are more than one character \n",
    "    cleaned_single = [word for word in stemmed_words if len(word) > 1]\n",
    "    #detokenize the words\n",
    "    cleaned_text = TreebankWordDetokenizer().detokenize(cleaned_single)\n",
    "    \n",
    "    #return the cleaned note\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755745f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#clean the notes\n",
    "labeled_notes.text = labeled_notes.text.apply(Cleaning_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the labeled notes after cleaning \n",
    "labeled_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22894b0",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0934d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the notes and labels \n",
    "X = labeled_notes.text\n",
    "y = labeled_notes.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae889a1",
   "metadata": {},
   "source": [
    "<h3>Approach 1 (TF-IDF)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing set \n",
    "X_train, X_test, y_train, y_test = train_test_split( X,y , random_state=42,test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adb0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    '''    \n",
    "    tokenizes a note \n",
    "\n",
    "    Args:\n",
    "        text (str): a note to be tokenized\n",
    " \n",
    "    Returns:\n",
    "        tokens: tokenized note \n",
    "    \n",
    "    '''\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a td-idf vectorizer \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),lowercase = False, min_df=3 ,tokenizer = tokenizer)\n",
    "#apply the td-idf vectorizer \n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVR_clf(x_train, y_train):\n",
    "    '''\n",
    "    fit SVR classifier on training data\n",
    "\n",
    "    Args:\n",
    "        x_train (str): X training set (text)\n",
    "        y_train (int): y training set (label)\n",
    " \n",
    "    Returns:\n",
    "        clf: fitted SVR classifier \n",
    "    \n",
    "    '''\n",
    "    clf = SVC(random_state=42 ,C=0.1,kernel='linear')\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def NB_clf(x_train, y_train):\n",
    "    '''\n",
    "    fit Naive Bayes classifier on training data\n",
    "\n",
    "    Args:\n",
    "        x_train (str): X training set (text)\n",
    "        y_train (int): y training set (label)\n",
    " \n",
    "    Returns:\n",
    "        clf: fitted NB classifier \n",
    "    \n",
    "    '''\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def LR_clf(x_train, y_train):\n",
    "    '''\n",
    "    fit Logistic Regression classifier on training data\n",
    "\n",
    "    Args:\n",
    "        x_train (str): X training set (text)\n",
    "        y_train (int): y training set (label)\n",
    " \n",
    "    Returns:\n",
    "        clf: fitted LR classifier \n",
    "    \n",
    "    '''\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf \n",
    "\n",
    "def evaluation(X_set,y_true,clf):\n",
    "    '''\n",
    "    predict using the fitted classifier and evaluate the performance\n",
    "\n",
    "    Args:\n",
    "        X_set (str): X training set to be used for prediction\n",
    "        y_true (int): y training set (label) \n",
    "        clf: fitted classifier\n",
    " \n",
    "    Returns:\n",
    "        prints classifcation evaluation report \n",
    "    \n",
    "    '''\n",
    "    y_pred = clf.predict(X_set)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the SVM classifier using td-idf vectors as input& get \n",
    "#the evaluation results on training and testing set\n",
    "SVR_tdidf = SVR_clf(X_train, y_train)\n",
    "print(\"training set report:\")\n",
    "evaluation(X_train,y_train,SVR_tdidf)\n",
    "print(\"testing set report\")\n",
    "evaluation(X_test,y_test,SVR_tdidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35585fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "NB_tdidf = NB_clf(X_train, y_train)\n",
    "#train the Naive bayes classifier using td-idf vectors as input\n",
    "#and get the evaluation results on training and testing set\n",
    "print(\"training set report\")\n",
    "evaluation(X_train,y_train,NB_tdidf)\n",
    "print(\"testing set report\")\n",
    "evaluation(X_test,y_test,NB_tdidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the logistic regression classifier using td-idf vectors as input\n",
    "#and get the evaluation results on training and testing set\n",
    "LR_tdidf = LR_clf(X_train, y_train)\n",
    "print(\"training set report\")\n",
    "evaluation(X_train,y_train,LR_tdidf)\n",
    "print(\"testing set report\")\n",
    "evaluation(X_test,y_test,LR_tdidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8415e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the the words from the td-idf vectorizer as the index\n",
    "# and the coefficents of fitted logistic regression classifier as the values in order to obtain the \n",
    "#top words that contributed to the classifcation of the positive class\n",
    "#in this instance diabetes type 1 is the positive class. \n",
    "important_tokens = pd.DataFrame(data=LR_tdidf.coef_[0],index=vectorizer.get_feature_names_out(),\n",
    "columns=['coefficient'])\n",
    "#sort values \n",
    "top__coeff = important_tokens.sort_values(by=['coefficient'], ascending=False).head(15)\n",
    "#reset index \n",
    "top__coeff.reset_index(inplace=True)\n",
    "# figure Size\n",
    "fig, ax = plt.subplots(figsize =(16, 9))\n",
    "# horizontal bar Plot\n",
    "ax.barh(top__coeff['index'], top__coeff.coefficient)\n",
    "ax.set_title('Feature importance of the top 15-ranked using coefficients of Logistic Regression for Type 1 Diabetes',\n",
    "             loc ='center', fontsize =16)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('Importance')\n",
    "#the labels were reversed to generate the diabetes type 2 top words (can be found in the report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51655b12",
   "metadata": {},
   "source": [
    "<h3>Approach 2 (Word Embeddings)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c857b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split( X,y , random_state=42,test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12baad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#split the training set into sentences \n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "#train a word2vec model to generate word embeddings \n",
    "w2v_model = Word2Vec(sentences, vector_size=300, window=50, min_count=10, workers=4 ,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    '''\n",
    "    vectorize sentences using word2vec\n",
    "\n",
    "    Args:\n",
    "        sentence (str): X training set (text) \n",
    " \n",
    "    Returns:\n",
    "        words_vectors: vectorized words \n",
    "    \n",
    "    '''\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    words_vectors = words_vecs.mean(axis=0)\n",
    "    return words_vectors\n",
    "\n",
    "#apply vectorize to preprocess the notes\n",
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scaler \n",
    "scaler = MinMaxScaler()\n",
    "#apply min-max scaler which scale the values between 0 and 1\n",
    "#as algorithms like naive bayes don't allow negative values \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the SVM classifier using word embeddings vectors as input\n",
    "#and get the evaluation results on training and testing set\n",
    "SVR_WordEmbedding = SVR_clf(X_train, y_train)\n",
    "print(\"training set report\")\n",
    "evaluation(X_train,y_train,SVR_WordEmbedding)\n",
    "print(\"testing set report\")\n",
    "evaluation(X_test,y_test,SVR_WordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11455c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the Naive Bayes classifier using word embeddings vectors as input\n",
    "#and get the evaluation results on training and testing set\n",
    "NB_WordEmbedding = NB_clf(X_train, y_train)\n",
    "print(\"training set report\")\n",
    "evaluation(X_train,y_train,NB_WordEmbedding)\n",
    "print(\"testing set report\")\n",
    "evaluation(X_test,y_test,NB_WordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed56ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the logistic regression classifier using word embeddings vectors as input\n",
    "#and get the evaluation results on training and testing set\n",
    "LR_WordEmbedding = LR_clf(X_train, y_train)\n",
    "print(\"training set report\")\n",
    "evaluation(X_train,y_train,LR_WordEmbedding)\n",
    "print(\"testing set report\")\n",
    "evaluation(X_test,y_test,LR_WordEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bf455",
   "metadata": {},
   "source": [
    "<h3>Approach 3 (Clinical BERT)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the dataset\n",
    "labeled_notes = labeled_notes.sample(frac =1) \n",
    "#get a sample of 7000 notes so it will be used in the modeling \n",
    "labeled_notes_subset = labeled_notes.sample(7000)\n",
    "#reset index as it was shuffled in the previous step\n",
    "labeled_notes_subset.reset_index(inplace=True)\n",
    "labeled_notes_subset = labeled_notes_subset[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f53b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tiny clinicalBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpie/tiny-clinicalbert\")\n",
    "#load the tiny clinicalBERT pretrained model and set it for binary classifcation\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlpie/tiny-clinicalbert\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e120a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing set\n",
    "train , test = train_test_split(labeled_notes_subset, shuffle=True ,test_size=0.2, random_state=42)\n",
    "#create a Dataset class type for training and testing set \n",
    "#transformers require the data to be in this form\n",
    "train_data = Dataset.from_pandas(train)\n",
    "test_data = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0607bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_tokenizer(df):\n",
    "    '''\n",
    "    toeknize the text using BERT tokenizer \n",
    "\n",
    "    Args:\n",
    "        df (str): X training set (text) \n",
    " \n",
    "    Returns:\n",
    "        tokenized_text: tokenized notes\n",
    "    \n",
    "    '''\n",
    "    tokenized_text = tokenizer(df['text'], padding=True, \n",
    "    max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train_data.map(BERT_tokenizer, batched=True)\n",
    "tokenized_test = test_data.map(BERT_tokenizer, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    '''\n",
    "    toeknize the text using BERT tokenizer \n",
    "\n",
    "    Args:\n",
    "        eval_pred: variable that contain the true labels and raw predictions of the transformer(logits) \n",
    " \n",
    "    Returns:\n",
    "        result: accuracy evaluation result \n",
    "    \n",
    "    '''\n",
    "    logits, labels = eval_pred\n",
    "    #return the label of the highest value(Probability of the class)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    #compute the accuracy during the training \n",
    "    result = metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the training arguments to fine tune the pretrained model for the classifcation task\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "#pass the model, dataset, hyperparameters, data collector and metrices into the trainer for training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00819154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "trainer.save_model('BERTdiabetes_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df074ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using the model on the training set\n",
    "y_pred_train = trainer.predict(tokenized_train)\n",
    "#return the label of the highest value(Probability of the class)\n",
    "y_pred_train = np.argmax(y_pred_train.predictions, axis=-1)\n",
    "#convert the y_train into a list \n",
    "y_train = train['label'].tolist()\n",
    "eval_report_train = classification_report(y_train, y_pred_train)\n",
    "#get the evaluation metrics \n",
    "print(\"training set report\")\n",
    "print(eval_report_train)\n",
    "\n",
    "\n",
    "\n",
    "#predict using the model on the training set\n",
    "y_pred = trainer.predict(tokenized_test)\n",
    "#return the label of the highest value(Probability of the class)\n",
    "y_pred = np.argmax(y_pred.predictions, axis=-1)\n",
    "#convert the y_test into a list \n",
    "y_test = test['label'].tolist()\n",
    "#get the evaluation metrics \n",
    "eval_report = classification_report(y_test, y_pred)\n",
    "print(\"testing set report\")\n",
    "print(eval_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ee216",
   "metadata": {},
   "source": [
    "<h3>Acknowledgments</h3>\n",
    "I would like to express my gratitude to: <br>\n",
    "1-StackOverFlow Community for there various answers to problems i faced during coding<br>\n",
    "2-Hugging face documentation & community<br>\n",
    "3-Neri Van Otten for her word2vec tutorial: https://spotintelligence.com/2023/02/15/word2vec-for-text-classification <br>\n",
    "4-Ray for transformers tutorial https://docs.ray.io/en/latest/train/getting-started-transformers.html <br> \n",
    "5-MIMIC IV contributors https://physionet.org/content/mimiciv/2.2/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
